<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>jax._src.nn.functions &#8212; netket v3.0 documentation</title>
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/jumbo-style.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/css/rtd_theme.css" />
    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/rtd_theme.js"></script>
    <link rel="author" title="About these documents" href="../../../../about.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../../../_static/bootstrap-sphinx.js "></script>

  </head><body>

  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../../../index.html"><span><img src="../../../../_static/logonav.png"></span>
          NetKet</a>
        <span class="navbar-text navbar-version pull-left"><b></b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../../../getting_started.html">Get Started</a></li>
                <li><a href="../../../../docs/getting_started.html">Documentation</a></li>
                <li><a href="../../../../tutorials.html">Tutorials</a></li>
                <li><a href="../../../../citing.html">Citing NetKet</a></li>
                <li><a href="../../../../about.html">About</a></li>
                <li><a href="https://github.com/netket/netket"><i class="fab fa-github" aria-hidden="true"></i></a></li>
                <li><a href="https://twitter.com/NetKetOrg"><i class="fab fa-twitter" aria-hidden="true"></i></a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="../../../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="body col-md-12 content" role="main">
      
  <h1>Source code for jax._src.nn.functions</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2019 Google LLC</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     https://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Shared neural network activations and other functions.&quot;&quot;&quot;</span>


<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">custom_jvp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">core</span>
<span class="kn">from</span> <span class="nn">jax.core</span> <span class="kn">import</span> <span class="n">AxisName</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">util</span>
<span class="kn">from</span> <span class="nn">jax.scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
<span class="kn">from</span> <span class="nn">jax.scipy.special</span> <span class="kn">import</span> <span class="n">logsumexp</span> <span class="k">as</span> <span class="n">_logsumexp</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">Array</span> <span class="o">=</span> <span class="n">Any</span>

<span class="c1"># activations</span>

<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.relu.html#netket.nn.relu">[docs]</a><span class="nd">@custom_jvp</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rectified linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{relu}(x) = \max(x, 0)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>
<span class="n">relu</span><span class="o">.</span><span class="n">defjvps</span><span class="p">(</span><span class="k">lambda</span> <span class="n">g</span><span class="p">,</span> <span class="n">ans</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">lax</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">lax</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>

<div class="viewcode-block" id="softplus"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.softplus.html#netket.nn.softplus">[docs]</a><span class="k">def</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Softplus activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{softplus}(x) = \log(1 + e^x)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">logaddexp</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span></div>

<div class="viewcode-block" id="soft_sign"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.soft_sign.html#netket.nn.soft_sign">[docs]</a><span class="k">def</span> <span class="nf">soft_sign</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Soft-sign activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{soft\_sign}(x) = \frac{x}{|x| + 1}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.sigmoid.html#netket.nn.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{sigmoid}(x) = \frac{1}{1 + e^{-x}}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;SiLU activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{silu}(x) = x \cdot \mathrm{sigmoid}(x) = \frac{x}{1 + e^{-x}}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">swish</span> <span class="o">=</span> <span class="n">silu</span>

<div class="viewcode-block" id="log_sigmoid"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.log_sigmoid.html#netket.nn.log_sigmoid">[docs]</a><span class="k">def</span> <span class="nf">log_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Log-sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{log\_sigmoid}(x) = \log(\mathrm{sigmoid}(x)) = -\log(1 + e^{-x})</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="o">-</span><span class="n">softplus</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span></div>

<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.elu.html#netket.nn.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Exponential linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{elu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha \left(\exp(x) - 1\right), &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    alpha : scalar or array of alpha values (default: 1.0)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">safe_x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">safe_x</span><span class="p">))</span></div>

<span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Leaky rectified linear unit activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{leaky\_relu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x \ge 0\\</span>
<span class="sd">      \alpha x, &amp; x &lt; 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  where :math:`\alpha` = :code:`negative_slope`.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    negative_slope : array or scalar specifying the negative slope (default: 0.01)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">negative_slope</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hard_tanh</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard :math:`\mathrm{tanh}` activation function.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_tanh}(x) = \begin{cases}</span>
<span class="sd">      -1, &amp; x &lt; -1\\</span>
<span class="sd">      x, &amp; -1 \le x \le 1\\</span>
<span class="sd">      1, &amp; 1 &lt; x</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>

<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.celu.html#netket.nn.celu">[docs]</a><span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Continuously-differentiable exponential linear unit activation.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{celu}(x) = \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha \left(\exp(\frac{x}{\alpha}) - 1\right), &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  For more information, see</span>
<span class="sd">  `Continuously Differentiable Exponential Linear Units</span>
<span class="sd">  &lt;https://arxiv.org/pdf/1704.07483.pdf&gt;`_.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    alpha : array or scalar (default: 1.0)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">expm1</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">alpha</span><span class="p">))</span></div>

<span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Scaled exponential linear unit activation.</span>

<span class="sd">  Computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{selu}(x) = \lambda \begin{cases}</span>
<span class="sd">      x, &amp; x &gt; 0\\</span>
<span class="sd">      \alpha e^x - \alpha, &amp; x \le 0</span>
<span class="sd">    \end{cases}</span>

<span class="sd">  where :math:`\lambda = 1.0507009873554804934193349852946` and</span>
<span class="sd">  :math:`\alpha = 1.6732632423543772848170429916717`.</span>

<span class="sd">  For more information, see</span>
<span class="sd">  `Self-Normalizing Neural Networks</span>
<span class="sd">  &lt;https://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf&gt;`_.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.6732632423543772848170429916717</span>
  <span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0507009873554804934193349852946</span>
  <span class="k">return</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.gelu.html#netket.nn.gelu">[docs]</a><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">approximate</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gaussian error linear unit activation function.</span>

<span class="sd">  If ``approximate=False``, computes the element-wise function:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{gelu}(x) = \frac{x}{2} \left(1 + \mathrm{erf} \left(</span>
<span class="sd">      \frac{x}{\sqrt{2}} \right) \right)</span>

<span class="sd">  If ``approximate=True``, uses the approximate formulation of GELU:</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{gelu}(x) = \frac{x}{2} \left(1 + \mathrm{tanh} \left(</span>
<span class="sd">      \sqrt{\frac{2}{\pi}} \left(x + 0.044715 x^3 \right) \right) \right)</span>

<span class="sd">  For more information, see `Gaussian Error Linear Units (GELUs)</span>
<span class="sd">  &lt;https://arxiv.org/abs/1606.08415&gt;`_, section 2.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    approximate: whether to use the approximate or exact formulation.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">approximate</span><span class="p">:</span>
    <span class="n">sqrt_2_over_pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">sqrt_2_over_pi</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">3</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">cdf</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">lax</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.glu.html#netket.nn.glu">[docs]</a><span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Gated linear unit activation function.</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis along which the split should be computed (default: -1)</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span>
  <span class="k">assert</span> <span class="n">size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;axis size must be divisible by 2&quot;</span>
  <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span></div>

<span class="c1"># other functions</span>

<span class="n">logsumexp</span> <span class="o">=</span> <span class="n">_logsumexp</span>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.log_softmax.html#netket.nn.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Log-Softmax function.</span>

<span class="sd">  Computes the logarithm of the :code:`softmax` function, which rescales</span>
<span class="sd">  elements to the range :math:`[-\infty, 0)`.</span>

<span class="sd">  .. math ::</span>
<span class="sd">    \mathrm{log\_softmax}(x) = \log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>
<span class="sd">    \right)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis or axes along which the :code:`log_softmax` should be</span>
<span class="sd">      computed. Either an integer or a tuple of integers.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">shifted</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">shifted</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">shifted</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span></div>

<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../../docs/_generated/nn/netket.nn.softmax.html#netket.nn.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Softmax function.</span>

<span class="sd">  Computes the function which rescales elements to the range :math:`[0, 1]`</span>
<span class="sd">  such that the elements along :code:`axis` sum to :math:`1`.</span>

<span class="sd">  .. math ::</span>
<span class="sd">    \mathrm{softmax}(x) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">    axis: the axis or axes along which the softmax should be computed. The</span>
<span class="sd">      softmax output summed across these dimensions should sum to :math:`1`.</span>
<span class="sd">      Either an integer or a tuple of integers.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">unnormalized</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">lax</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span>
  <span class="k">return</span> <span class="n">unnormalized</span> <span class="o">/</span> <span class="n">unnormalized</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span>
              <span class="n">axis</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
              <span class="n">mean</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">variance</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Array</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">epsilon</span><span class="p">:</span> <span class="n">Array</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;Normalizes an array by subtracting mean and dividing by sqrt(var).&quot;&quot;&quot;</span>
  <span class="k">if</span> <span class="n">mean</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">variance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># this definition is traditionally seen as less accurate than jnp.var&#39;s</span>
    <span class="c1"># mean((x - mean(x))**2) but may be faster and even, given typical</span>
    <span class="c1"># activation distributions and low-precision arithmetic, more accurate</span>
    <span class="c1"># when used in neural network normalization layers</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">AxisName</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sd">&quot;&quot;&quot;One-hot encodes the given indicies.</span>

<span class="sd">  Each index in the input ``x`` is encoded as a vector of zeros of length</span>
<span class="sd">  ``num_classes`` with the element at ``index`` set to one::</span>

<span class="sd">    &gt;&gt;&gt; jax.nn.one_hot(jnp.array([0, 1, 2]), 3)</span>
<span class="sd">    DeviceArray([[1., 0., 0.],</span>
<span class="sd">                  [0., 1., 0.],</span>
<span class="sd">                  [0., 0., 1.]], dtype=float32)</span>

<span class="sd">  Indicies outside the range [0, num_classes) will be encoded as zeros::</span>

<span class="sd">    &gt;&gt;&gt; jax.nn.one_hot(jnp.array([-1, 3]), 3)</span>
<span class="sd">    DeviceArray([[0., 0., 0.],</span>
<span class="sd">                 [0., 0., 0.]], dtype=float32)</span>

<span class="sd">  Args:</span>
<span class="sd">    x: A tensor of indices.</span>
<span class="sd">    num_classes: Number of classes in the one-hot dimension.</span>
<span class="sd">    dtype: optional, a float dtype for the returned values (default float64 if</span>
<span class="sd">      jax_enable_x64 is true, otherwise float32).</span>
<span class="sd">    axis: the axis or axes along which the function should be</span>
<span class="sd">      computed.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">num_classes</span> <span class="o">=</span> <span class="n">core</span><span class="o">.</span><span class="n">concrete_or_error</span><span class="p">(</span>
      <span class="nb">int</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span>
      <span class="s2">&quot;The error arose in jax.nn.one_hot argument `num_classes`.&quot;</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">dtypes</span><span class="o">.</span><span class="n">canonicalize_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">output_pos_axis</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">canonicalize_axis</span><span class="p">(</span><span class="n">axis</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
    <span class="n">axis_size</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">psum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">num_classes</span> <span class="o">!=</span> <span class="n">axis_size</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Expected num_classes to match the size of axis </span><span class="si">{</span><span class="n">axis</span><span class="si">}</span><span class="s2">, &quot;</span>
                       <span class="sa">f</span><span class="s2">&quot;but </span><span class="si">{</span><span class="n">num_classes</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">axis_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> <span class="kn">from</span> <span class="bp">None</span>
    <span class="n">axis_idx</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">axis_index</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span> <span class="o">==</span> <span class="n">axis_idx</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">axis</span> <span class="o">=</span> <span class="n">operator</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
  <span class="n">lhs</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="n">axis</span><span class="p">,))</span>
  <span class="n">rhs_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span>
  <span class="n">rhs_shape</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">output_pos_axis</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
  <span class="n">rhs</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">broadcast_in_dim</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                             <span class="n">rhs_shape</span><span class="p">,</span> <span class="p">(</span><span class="n">output_pos_axis</span><span class="p">,))</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">lhs</span> <span class="o">==</span> <span class="n">rhs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Rectified Linear Unit 6 activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{relu6}(x) = \min(\max(x, 0), 6)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mf">6.</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">hard_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard Sigmoid activation function.</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_sigmoid}(x) = \frac{\mathrm{relu6}(x + 3)}{6}</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">relu6</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">3.</span><span class="p">)</span> <span class="o">/</span> <span class="mf">6.</span>

<span class="k">def</span> <span class="nf">hard_silu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Array</span><span class="p">:</span>
  <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Hard SiLU activation function</span>

<span class="sd">  Computes the element-wise function</span>

<span class="sd">  .. math::</span>
<span class="sd">    \mathrm{hard\_silu}(x) = x \cdot \mathrm{hard\_sigmoid}(x)</span>

<span class="sd">  Args:</span>
<span class="sd">    x : input array</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">hard_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">hard_swish</span> <span class="o">=</span> <span class="n">hard_silu</span>
</pre></div>

    </div>
      
  </div>
</div>

<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2019-2021, The Netket authors - All rights reserved.<br/>
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.5.3.<br/>
    </p>
  </div>
</footer>

<script type="text/javascript">
    jQuery(function () {
        SphinxRtdTheme.Navigation.enable(true);
      })
</script>

<!-- Temporary footer
<div class="footer-wip">
  <div class="footer-wip-content">
    This documentation refers to an unreleased version of Netket.
  </div>
</div>
-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-118013987-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-118013987-1');
</script>

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "url": "https://www.netket.org",
  "name": "NetKet",
  "founder": "Giuseppe Carleo",
  "foundingDate": "2018-04-24",
  "foundingLocation" : "New York",
  "logo": "https://www.netket.org/img/logo_small.jpg",
  "sameAs": [
    "https://twitter.com/NetKetOrg",
    "https://github.com/NetKet/netket"
  ],
  "description" : "Netket is an open-source project delivering cutting-edge
  methods for the study of many-body quantum systems with artificial neural
  networks and machine learning techniques."
}
</script>

  </body>
</html>